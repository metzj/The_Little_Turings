{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from misc_helpers import *\n",
    "from plot_functions import *\n",
    "from ml_math import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y = (y+1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(y, x, ratio, myseed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(myseed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.6\n",
    "x_train, x_test, y_train, y_test = split_data(y, tX, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t, l = 1):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0+np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    s = 0\n",
    "    for n in range(len(y)):\n",
    "        s += np.log(1 + np.exp(np.dot(tx[n,:].T,w))) - y[n]*np.dot(tx[n,:].T,w)\n",
    "    return s \n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.dot(tx.T,sigmoid(np.dot(tx,w))-y)\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    N = len(y)\n",
    "    S = np.eye(N)\n",
    "    for n in range(N):\n",
    "        pred = sigmoid(np.dot(tx[n,:].T,w))\n",
    "        S[n,n] = pred*(1-pred)\n",
    "    return tx.T.dot(S.dot(tx))\n",
    "\n",
    "def logistic_regression(y, tx, w, newton = False):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    if newton:\n",
    "        hess = calculate_hessian(y, tx, w)\n",
    "    else:\n",
    "        hess = 0\n",
    "    return loss, gradient, hess\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_, newton = False):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss, gradient, hess = logistic_regression(y, tx, w, newton)\n",
    "    loss += lambda_/2*np.linalg.norm(w)**2\n",
    "    gradient += lambda_*w\n",
    "    if newton:\n",
    "        hess += lambda_*np.eye(len(w)).dot(w)\n",
    "    else:\n",
    "        hess = 0\n",
    "    return loss, gradient, hess\n",
    "\n",
    "def logistic_regression_ADAM(y , tx, lambda_, maxit, verbose = False):\n",
    "    \"\"\"return w using ADAM\"\"\"\n",
    "    n , p =np.shape(tx)\n",
    "    w = np.zeros(p)\n",
    "    w_prev = w\n",
    "    alpha = 0.1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1E-8\n",
    "    m_prev = 0\n",
    "    v_prev = 0\n",
    "    for k in range(maxit):\n",
    "        g = calculate_gradient(y, tx, w_prev) + lambda_ * w\n",
    "        m = beta1*m_prev + (1-beta1)*g\n",
    "        v = beta2*v_prev + (1-beta2)*g**2\n",
    "        m_hat = m/(1-beta1)\n",
    "        v_hat = v/(1-beta2)\n",
    "        H = np.sqrt(v_hat)+eps\n",
    "        w_next = w - alpha*m_hat/H\n",
    "        \n",
    "        w = w_next\n",
    "        w_prev = w\n",
    "        m_prev = m\n",
    "        v_prev = v\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        #if not k%10 and verbose:\n",
    "        if verbose:\n",
    "            print ('%d : loss = %f, norm(g) = %f'%(k,loss,np.linalg.norm(w)) )\n",
    "    return w, loss\n",
    "\n",
    "def logistic_regression_GD(y , tx, gamma, lambda_, maxit, verbose = False):\n",
    "    \"\"\"return w using ADAM\"\"\"\n",
    "    n , p =np.shape(tx)\n",
    "    w = np.zeros(p)\n",
    "    \n",
    "    for k in range(maxit):\n",
    "        g = calculate_gradient(y, tx, w) + lambda_ * w\n",
    "        w = w - gamma*g\n",
    "        \n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        #if not k%10 and verbose:\n",
    "        if verbose:\n",
    "            print ('%d : loss = %f, norm(g) = %f'%(k,loss,np.linalg.norm(w)) )\n",
    "    return w, loss\n",
    "\n",
    "def logistic_regression_SGD(y , tx, lambda_, maxit):\n",
    "    \"\"\"return w using ADAM\"\"\"\n",
    "    n , p =np.shape(tx)\n",
    "    w = np.zeros(p)\n",
    "    w_prev = w\n",
    "    alpha = 0.1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1E-8\n",
    "    m_prev = 0\n",
    "    v_prev = 0\n",
    "    for k in range(maxit):\n",
    "        g = calculate_gradient(y, tx, w_prev) + lambda_ * w\n",
    "        m = beta1*m_prev + (1-beta1)*g\n",
    "        v = beta2*v_prev + (1-beta2)*g**2\n",
    "        m_hat = m/(1-beta1)\n",
    "        v_hat = v/(1-beta2)\n",
    "        H = np.sqrt(v_hat)+eps\n",
    "        w_next = w - alpha*m_hat/H\n",
    "        \n",
    "        w = w_next\n",
    "        w_prev = w\n",
    "        m_prev = m\n",
    "        v_prev = v\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : loss = 103839.673495, norm(g) = 0.547723\n",
      "1 : loss = 103663.193372, norm(g) = 1.279493\n",
      "2 : loss = 103459.745190, norm(g) = 2.116993\n",
      "3 : loss = 103238.781133, norm(g) = 3.027679\n",
      "4 : loss = 103006.516402, norm(g) = 3.991741\n",
      "5 : loss = 102767.364769, norm(g) = 4.991813\n",
      "6 : loss = 102524.589742, norm(g) = 6.015421\n",
      "7 : loss = 102280.660509, norm(g) = 7.053083\n",
      "8 : loss = 102037.468558, norm(g) = 8.096447\n",
      "9 : loss = 101796.469349, norm(g) = 9.138420\n",
      "10 : loss = 101558.780402, norm(g) = 10.173553\n",
      "11 : loss = 101325.252642, norm(g) = 11.197767\n",
      "12 : loss = 101096.524847, norm(g) = 12.208012\n",
      "13 : loss = 100873.066857, norm(g) = 13.202117\n",
      "14 : loss = 100655.214332, norm(g) = 14.178719\n",
      "15 : loss = 100443.196326, norm(g) = 15.137138\n",
      "16 : loss = 100237.156486, norm(g) = 16.077175\n",
      "17 : loss = 100037.169193, norm(g) = 16.998877\n",
      "18 : loss = 99843.252204, norm(g) = 17.902350\n",
      "19 : loss = 99655.376857, norm(g) = 18.787675\n",
      "20 : loss = 99473.476522, norm(g) = 19.654960\n",
      "21 : loss = 99297.453863, norm(g) = 20.504427\n",
      "22 : loss = 99127.187247, norm(g) = 21.336446\n",
      "23 : loss = 98962.536307, norm(g) = 22.151487\n",
      "24 : loss = 98803.346614, norm(g) = 22.950033\n",
      "25 : loss = 98649.453574, norm(g) = 23.732501\n",
      "26 : loss = 98500.685755, norm(g) = 24.499211\n",
      "27 : loss = 98356.867780, norm(g) = 25.250387\n",
      "28 : loss = 98217.822718, norm(g) = 25.986171\n",
      "29 : loss = 98083.373948, norm(g) = 26.706661\n",
      "30 : loss = 97953.346551, norm(g) = 27.411929\n",
      "31 : loss = 97827.568334, norm(g) = 28.102046\n",
      "32 : loss = 97705.870589, norm(g) = 28.777100\n",
      "33 : loss = 97588.088665, norm(g) = 29.437213\n",
      "34 : loss = 97474.062404, norm(g) = 30.082557\n",
      "35 : loss = 97363.636481, norm(g) = 30.713347\n",
      "36 : loss = 97256.660641, norm(g) = 31.329844\n",
      "37 : loss = 97152.989878, norm(g) = 31.932339\n",
      "38 : loss = 97052.484568, norm(g) = 32.521149\n",
      "39 : loss = 96955.010592, norm(g) = 33.096611\n",
      "40 : loss = 96860.439408, norm(g) = 33.659075\n",
      "41 : loss = 96768.648050, norm(g) = 34.208903\n",
      "42 : loss = 96679.519032, norm(g) = 34.746461\n",
      "43 : loss = 96592.940175, norm(g) = 35.272111\n",
      "44 : loss = 96508.804403, norm(g) = 35.786210\n",
      "45 : loss = 96427.009532, norm(g) = 36.289106\n",
      "46 : loss = 96347.458068, norm(g) = 36.781143\n",
      "47 : loss = 96270.057032, norm(g) = 37.262662\n",
      "48 : loss = 96194.717789, norm(g) = 37.734003\n",
      "49 : loss = 96121.355894, norm(g) = 38.195502\n",
      "50 : loss = 96049.890938, norm(g) = 38.647486\n",
      "51 : loss = 95980.246411, norm(g) = 39.090274\n",
      "52 : loss = 95912.349548, norm(g) = 39.524171\n",
      "53 : loss = 95846.131183, norm(g) = 39.949469\n",
      "54 : loss = 95781.525562, norm(g) = 40.366449\n",
      "55 : loss = 95718.470144, norm(g) = 40.775385\n",
      "56 : loss = 95656.905382, norm(g) = 41.176541\n",
      "57 : loss = 95596.774510, norm(g) = 41.570181\n",
      "58 : loss = 95538.023345, norm(g) = 41.956567\n",
      "59 : loss = 95480.600128, norm(g) = 42.335959\n",
      "60 : loss = 95424.455391, norm(g) = 42.708619\n",
      "61 : loss = 95369.541874, norm(g) = 43.074802\n",
      "62 : loss = 95315.814450, norm(g) = 43.434758\n",
      "63 : loss = 95263.230090, norm(g) = 43.788727\n",
      "64 : loss = 95211.747814, norm(g) = 44.136935\n",
      "65 : loss = 95161.328643, norm(g) = 44.479595\n",
      "66 : loss = 95111.935531, norm(g) = 44.816904\n",
      "67 : loss = 95063.533273, norm(g) = 45.149042\n",
      "68 : loss = 95016.088384, norm(g) = 45.476179\n",
      "69 : loss = 94969.568973, norm(g) = 45.798472\n",
      "70 : loss = 94923.944616, norm(g) = 46.116072\n",
      "71 : loss = 94879.186227, norm(g) = 46.429122\n",
      "72 : loss = 94835.265960, norm(g) = 46.737760\n",
      "73 : loss = 94792.157123, norm(g) = 47.042124\n",
      "74 : loss = 94749.834118, norm(g) = 47.342345\n",
      "75 : loss = 94708.272400, norm(g) = 47.638555\n",
      "76 : loss = 94667.448445, norm(g) = 47.930877\n",
      "77 : loss = 94627.339727, norm(g) = 48.219432\n",
      "78 : loss = 94587.924700, norm(g) = 48.504333\n",
      "79 : loss = 94549.182762, norm(g) = 48.785685\n",
      "80 : loss = 94511.094224, norm(g) = 49.063588\n",
      "81 : loss = 94473.640260, norm(g) = 49.338134\n",
      "82 : loss = 94436.802854, norm(g) = 49.609410\n",
      "83 : loss = 94400.564743, norm(g) = 49.877497\n",
      "84 : loss = 94364.909364, norm(g) = 50.142475\n",
      "85 : loss = 94329.820798, norm(g) = 50.404419\n",
      "86 : loss = 94295.283733, norm(g) = 50.663400\n",
      "87 : loss = 94261.283427, norm(g) = 50.919491\n",
      "88 : loss = 94227.805678, norm(g) = 51.172759\n",
      "89 : loss = 94194.836806, norm(g) = 51.423271\n",
      "90 : loss = 94162.363632, norm(g) = 51.671093\n",
      "91 : loss = 94130.373464, norm(g) = 51.916285\n",
      "92 : loss = 94098.854073, norm(g) = 52.158907\n",
      "93 : loss = 94067.793673, norm(g) = 52.399016\n",
      "94 : loss = 94037.180902, norm(g) = 52.636669\n",
      "95 : loss = 94007.004790, norm(g) = 52.871917\n",
      "96 : loss = 93977.254744, norm(g) = 53.104813\n",
      "97 : loss = 93947.920524, norm(g) = 53.335407\n",
      "98 : loss = 93918.992225, norm(g) = 53.563749\n",
      "99 : loss = 93890.460270, norm(g) = 53.789886\n"
     ]
    }
   ],
   "source": [
    "# Validate logistic_regression_ADAM\n",
    "tx_norm = normalize(x_train)\n",
    "lambda_ = 0.5\n",
    "w1 , loss= logistic_regression_ADAM(y_train ,tx_norm, lambda_, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : loss = 93732.372824, norm(g) = 55.747337\n",
      "1 : loss = 92278.556897, norm(g) = 69.191352\n",
      "2 : loss = 91517.365786, norm(g) = 76.163061\n",
      "3 : loss = 90982.617400, norm(g) = 81.425982\n",
      "4 : loss = 90572.852465, norm(g) = 85.867118\n",
      "5 : loss = 90246.383620, norm(g) = 89.705273\n",
      "6 : loss = 89980.591184, norm(g) = 93.035721\n",
      "7 : loss = 89761.207437, norm(g) = 95.926989\n",
      "8 : loss = 89578.387118, norm(g) = 98.437138\n",
      "9 : loss = 89424.947175, norm(g) = 100.616561\n",
      "10 : loss = 89295.451687, norm(g) = 102.508962\n",
      "11 : loss = 89185.680271, norm(g) = 104.152187\n",
      "12 : loss = 89092.293437, norm(g) = 105.579024\n",
      "13 : loss = 89012.609431, norm(g) = 106.817911\n",
      "14 : loss = 88944.448988, norm(g) = 107.893543\n",
      "15 : loss = 88886.023849, norm(g) = 108.827380\n",
      "16 : loss = 88835.854662, norm(g) = 109.638077\n",
      "17 : loss = 88792.709271, norm(g) = 110.341857\n",
      "18 : loss = 88755.555522, norm(g) = 110.952824\n",
      "19 : loss = 88723.524607, norm(g) = 111.483235\n",
      "20 : loss = 88695.882223, norm(g) = 111.943740\n",
      "21 : loss = 88672.005597, norm(g) = 112.343588\n",
      "22 : loss = 88651.364994, norm(g) = 112.690811\n",
      "23 : loss = 88633.508691, norm(g) = 112.992377\n",
      "24 : loss = 88618.050667, norm(g) = 113.254334\n",
      "25 : loss = 88604.660438, norm(g) = 113.481928\n",
      "26 : loss = 88593.054623, norm(g) = 113.679706\n",
      "27 : loss = 88582.989893, norm(g) = 113.851615\n",
      "28 : loss = 88574.257061, norm(g) = 114.001073\n",
      "29 : loss = 88566.676098, norm(g) = 114.131046\n",
      "30 : loss = 88560.091921, norm(g) = 114.244104\n",
      "31 : loss = 88554.370827, norm(g) = 114.342477\n",
      "32 : loss = 88549.397452, norm(g) = 114.428096\n",
      "33 : loss = 88545.072187, norm(g) = 114.502637\n",
      "34 : loss = 88541.308970, norm(g) = 114.567553\n",
      "35 : loss = 88538.033399, norm(g) = 114.624104\n",
      "36 : loss = 88535.181116, norm(g) = 114.673385\n",
      "37 : loss = 88532.696420, norm(g) = 114.716343\n",
      "38 : loss = 88530.531079, norm(g) = 114.753801\n",
      "39 : loss = 88528.643306, norm(g) = 114.786475\n",
      "40 : loss = 88526.996884, norm(g) = 114.814985\n",
      "41 : loss = 88525.560406, norm(g) = 114.839871\n",
      "42 : loss = 88524.306626, norm(g) = 114.861599\n",
      "43 : loss = 88523.211897, norm(g) = 114.880577\n",
      "44 : loss = 88522.255688, norm(g) = 114.897159\n",
      "45 : loss = 88521.420163, norm(g) = 114.911652\n",
      "46 : loss = 88520.689823, norm(g) = 114.924324\n",
      "47 : loss = 88520.051197, norm(g) = 114.935406\n",
      "48 : loss = 88519.492567, norm(g) = 114.945103\n",
      "49 : loss = 88519.003738, norm(g) = 114.953589\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.2\n",
    "w2 , loss= logistic_regression_GD(y_trai ,tx_norm, gamma, lambda_, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82362.48239936346\n",
      "87222.02568160382\n"
     ]
    }
   ],
   "source": [
    "tx_test_norm =normalize(x_test)\n",
    "print(calculate_loss((y_test+1)/2,tx_test_norm,w1))\n",
    "print(calculate_loss((y_test+1)/2,tx_test_norm,w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-5a4abf258794>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-5a4abf258794>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "    \n",
    "def cross_validation(y, x, k_fold, solver = 'LS',stoch = True,lambda_ = 0, maxit = 1):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    mse_tr = 0\n",
    "    mse_te = 0\n",
    "    #p = np.shape(x)[1]\n",
    "    #w0 = np.zeros(p)\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        # get k'th subgroup in test, others in train:\n",
    "        test_indices = k_indices[k]\n",
    "        train_indices = np.delete(k_indices,k,0).flatten()\n",
    "        x_tr = x[train_indices]\n",
    "        y_tr = y[train_indices]\n",
    "        x_te = x[test_indices]\n",
    "        y_te = y[test_indices]\n",
    "\n",
    "        # Least squares:\n",
    "        if solver == 'LS':\n",
    "            w, loss = least_squares(y_tr, x_tr)\n",
    "        elif solver == 'RR':\n",
    "            w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "        elif solver == 'LR':\n",
    "            \n",
    "        else:\n",
    "            raise('Error')\n",
    "\n",
    "        # calculate the loss for train and test data: \n",
    "        \n",
    "        loss_tr = compute_error(x_tr,y_tr, w)\n",
    "        loss_te = compute_error(x_te,y_te, w)\n",
    "    \n",
    "        mse_tr += loss_tr/k_fold\n",
    "        mse_te += loss_te/k_fold\n",
    "        \n",
    "    \n",
    "    return mse_tr, mse_te, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = normalize(y)\n",
    "tX_norm = normalize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_LS, loss_LS = least_squares(y_train,x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Regulated Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Choose the weight you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = w_GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'result/to_try.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, build_poly(tX_test,degree))\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(build_poly(tX_test,degree))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
