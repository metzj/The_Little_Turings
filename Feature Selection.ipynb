{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (implementations.py, line 62)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/jordanmetz/anaconda3/envs/ML_course/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4eeab23ba94c>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from implementations import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/jordanmetz/Documents/EPFL/MA3/The_Little_Turings/implementations.py\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    n = len(y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from misc_helpers import *\n",
    "from plot_functions import *\n",
    "from ml_math import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = normalize(y)\n",
    "tX_norm = normalize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_fold, solver = 'LS',lambda_ = 0):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    mse_tr = 0\n",
    "    mse_te = 0\n",
    "    for k in range(k_fold):\n",
    "        # get k'th subgroup in test, others in train:\n",
    "        test_indices = k_indices[k]\n",
    "        train_indices = np.delete(k_indices,k,0).flatten()\n",
    "        x_tr = x[train_indices]\n",
    "        y_tr = y[train_indices]\n",
    "        x_te = x[test_indices]\n",
    "        y_te = y[test_indices]\n",
    "\n",
    "        # Least squares:\n",
    "        if solver == 'LS':\n",
    "            w, loss = least_squares(y_tr, x_tr)\n",
    "        elif solver == 'RR':\n",
    "            w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "        else:\n",
    "            raise('Error')\n",
    "\n",
    "        # calculate the loss for train and test data: \n",
    "        loss_tr = compute_MSE(y_tr, x_tr, w)\n",
    "        loss_te = compute_MSE(y_te, x_te, w)\n",
    "    \n",
    "        mse_tr += loss_tr/k_fold\n",
    "        mse_te += loss_te/k_fold\n",
    "        \n",
    "    \n",
    "    return mse_tr, mse_te, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some crazy feature selection here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal\n",
    "cross_validation(y,tX,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal normalized\n",
    "cross_validation(y_norm,tX_norm,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree, linear = False):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    if linear == False:\n",
    "        D = len(x[0,:])\n",
    "        N = len(x[:,0])\n",
    "        new_x = np.ones((N,1)) #add bias\n",
    "        if degree>=1:\n",
    "            for i in range(1,degree+1):\n",
    "                new_x = np.append(new_x,x**i,axis=1) \n",
    "        return new_x\n",
    "    else:\n",
    "        m = np.zeros((len(x),degree+1))\n",
    "        for j in range(degree+1):\n",
    "            m[:,j] = x**j\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the noise level. Everything that we add should beat this error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only constant feature\n",
    "tX0 = build_poly(tX,0)\n",
    "print(cross_validation(y,tX0,4))\n",
    "#only constant feature\n",
    "tX0 = build_poly(tX_norm,0)\n",
    "print(cross_validation(y,tX0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test every degree for every feature\n",
    "n,p = np.shape(tX)\n",
    "loss = []\n",
    "good_feat = []\n",
    "min_loss = np.ones(p)*10000\n",
    "min_deg = np.zeros(p)\n",
    "for deg in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "    for i in range(p):\n",
    "        tX_ = build_poly(tX[:,i],deg, linear=True)\n",
    "        loss_tr,loss_te,w = cross_validation(y,tX_,5)\n",
    "        if loss_te<min_loss[i]:\n",
    "            min_loss[i] = loss_te\n",
    "            min_deg[i] = deg\n",
    "        if loss_te < 0.45:\n",
    "            good_feat.append((i,deg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all of these are good features\n",
    "id_min_loss = np.where(min_loss < 0.43)\n",
    "id_min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all good feature in tX0\n",
    "tX0 = build_poly(tX,0)\n",
    "for i in  id_min_loss[0]:\n",
    "    deg = int(min_deg[i])\n",
    "    tX0 = np.append(tX0, build_poly(tX[:,i], deg, linear=True),1)\n",
    "tX0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ridge regression:\n",
    "    D = np.shape(tx)[1]\n",
    "    N = np.shape(tx)[0]\n",
    "    w = np.linalg.solve(tx.transpose()@tx+2*N*lambda_*np.identity(D), tx.transpose()@y)\n",
    "    #Compute loss\n",
    "    e = y - tx @ w\n",
    "    N = len(y)\n",
    "    loss = 1/(2*N)*e@e\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10, 0, 30)\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "mse_te_min = 10000\n",
    "w0 = np.ones((p))\n",
    "for lambda_ in lambdas:\n",
    "    mse_tr, mse_te, w = cross_validation(y,tX0,5,solver = 'RR',lambda_ = lambda_)\n",
    "    rmse_tr = np.append(rmse_tr,mse_tr)\n",
    "    rmse_te = np.append(rmse_te,mse_te)\n",
    "    if mse_te < mse_te_min:\n",
    "        w0 = w\n",
    "        mse_te_min = mse_te\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Test for AIcrowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_test = normalize(tX_test)\n",
    "#concatenate all good feature in tX0\n",
    "tX_test_0 = build_poly(tX_test,0)\n",
    "for i in  id_min_loss[0]:\n",
    "    deg = int(min_deg[i])\n",
    "    tX_test_0 = np.append(tX_test_0, build_poly(tX_test[:,i], deg, linear=True),1)\n",
    "np.shape(tX_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'result/feature_selection_norm.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w0, tX_test_0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from Remi's conclusion. Which is to take only the feature.\\\n",
    "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 (the id in feature is one less)\\\n",
    "First, delete where there is undefined feature. We will use:\\\n",
    "2, 3, 6, 7, 9, 10, 11, 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see for each, which of the degree is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_feat = [2,3,6,7,9,10,11,13]\n",
    "\n",
    "for feat in good_feat:\n",
    "    for deg in range(10):\n",
    "        cross_validation(y, build_poly(tX[:,feat],), 5,solver = 'RR',lambda_ = lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's try to see if there is a $1/x$ relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_course] *",
   "language": "python",
   "name": "conda-env-ML_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
