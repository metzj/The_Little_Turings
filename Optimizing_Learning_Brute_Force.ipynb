{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from misc_helpers import *\n",
    "from plot_functions import *\n",
    "from ml_math import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Learning by \"brute force\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_data, x_data, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y_data+1)/2\n",
    "x = normalize(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best number of gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'New' build_poly used in the labs correction, the old one should also work but you're never sure\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that builds a matrix x to a polynomial matrix of of powers of x.\n",
    "    ----------------------------------------------------------------------------\n",
    "    Input:\n",
    "    - x         parameter, (nsamples, nfeature) np.array\n",
    "    - degree    number of dimension, if it is a float number, convert it into\n",
    "                an integer\n",
    "    Output:\n",
    "    - poly      polynomial matrix, (nsamples, (nfeature*degree) + 1), np.array\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, int(degree+1)):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "# Build_k_indices to do cross validation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that builds \"k_fold\" sets of randomly distributed indices for an \n",
    "    array of size y.shape[0]\n",
    "    ----------------------------------------------------------------------------\n",
    "    Input:\n",
    "    - y         array to be permuted in the future, (nsamples, 1) np.array\n",
    "    - k_fold    number of sets of same size to be build, integer\n",
    "    - seed      seed for the random number generator\n",
    "    Output:\n",
    "    - answ      k_folds set of randomly distributed indices, (k_fold, \n",
    "                nsamples/k_fold), np.array\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation_visualization(param, tested_parameter, rmse_tr, rmse_te, \n",
    "                                   graph_type = \"log\", label=\"\", color1 = \"r\", color2 = \"b\"):\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that plots the error in training and in test as a function of a\n",
    "    given parameter, then saves it with the name \"cross_validation\".\n",
    "    ----------------------------------------------------------------------------\n",
    "    Input:\n",
    "    - param             values of the parameter, array\n",
    "    - tested_parameter  name of the parameter, string\n",
    "    - rmse_tr           training error, array\n",
    "    - rmse_te           test error, array\n",
    "    - graph_type        choose between a loglog graph - \"log\" - and a semilogy\n",
    "                        - otherwise - graph, string, (Default = \"log\")\n",
    "    - label             gives additional information on the label, string\n",
    "                        (Default = \"\")\n",
    "    - color1            color of the train data (Default = \"r\")\n",
    "    - color2            color of the test data (Default = \"b\")\n",
    "    Output:\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    if (graph_type == \"log\"):\n",
    "        plt.loglog(param, rmse_tr, marker=\".\", color=color1, label='train error' + \" \" + label)\n",
    "        plt.loglog(param, rmse_te, marker=\".\", color=color2, label='test error' + \" \" + label)\n",
    "    else:\n",
    "        plt.semilogy(param, rmse_tr, marker=\".\", color=color1, label='train error' + \" \" + label)\n",
    "        plt.semilogy(param, rmse_te, marker=\".\", color=color2, label='test error' + \" \" + label)\n",
    "    plt.xlabel(tested_parameter)\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation_\" + tested_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, degree, gamma, lambda_, max_iters, method, w0):\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that implements the cross-validation given multiple parameters.\n",
    "    Returns the calculated, loss in both the training and test sets, and the\n",
    "    weights\n",
    "    ----------------------------------------------------------------------------\n",
    "    Input:\n",
    "    - y         \"measured\" objective function, (nsamples,1) np.array\n",
    "    - x         features, (nsamples,nfeatures) np.array\n",
    "    - k_indices k_fold sets of randomly distributed indices, (k_fold, \n",
    "                nsamples/k_fold), np.array\n",
    "    - k         number of the set to use as test set\n",
    "    - degree    number of dimension\n",
    "    - gamma     step size, scalar in ]0,1[\n",
    "    - lambda_   regularization parameter, scalar>0\n",
    "    - max_iters # of iterations after which the procedure will stop, int>0\n",
    "    - method    method to use in this cross-validation iteration, integer \n",
    "                between 0 to 6 (0 = least_square_GD, 1 = least_square_SGD,\n",
    "                2 = least_squares, 3 = ridge_regression, 4 = \n",
    "                logistic_regression, 5 = reg_logistic_regression, 6 = \n",
    "                new_ridge_regression_GD)\n",
    "    - w0\n",
    "    Output:\n",
    "    - w             obtained weights, (nfeatures,1) np.array\n",
    "    - loss_tr       loss on the training set, scalar\n",
    "    - loss_te       loss on the test set, scalar\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    loss_function = \"MSE\"\n",
    "    \n",
    "    if method == 0:\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        initial_w = np.ones(len(tx_tr[1,:])) * w0\n",
    "        w, loss_tr = least_squares_GD(y_tr, tx_tr, initial_w, max_iters, gamma, verbose=False)\n",
    "    if method == 1:\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        initial_w = np.ones(len(tx_tr[1,:])) * w0\n",
    "        w, loss_tr = least_squares_SGD(y_tr, tx_tr, initial_w, 1, max_iters, gamma, verbose=False)\n",
    "    if method == 2:\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        w, loss_tr = least_squares(y_tr, tx_tr)\n",
    "    if method == 3:\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        w, loss_tr = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    if method == 4:\n",
    "        loss_function = \"MSE\"\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        initial_w = np.ones(len(tx_tr[1,:])) * w0\n",
    "        w, loss_tr = logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma, verbose = False, use_SGD = True, batch_size = 1, loss_function = \"LL2\")\n",
    "    if method == 5:\n",
    "        loss_function = \"MSE\"\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        initial_w = np.ones(len(tx_tr[1,:])) * w0\n",
    "        w, loss_tr = reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gamma, verbose = False, use_SGD = True, batch_size = 1, loss_function = \"LL2\")\n",
    "    if method == 6:\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        initial_w = np.ones(len(tx_tr[1,:])) * w0\n",
    "        w, loss_tr = new_ridge_regression_GD(y_tr, tx_tr, initial_w, lambda_, gamma, max_iters, verbose=False)\n",
    "    \n",
    "    # calculate the loss for test data\n",
    "    loss_te = compute_loss(y_te, tx_te, w, loss_function)\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tests on all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all():\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that test all the variable parameters in a huge for loop.\n",
    "    Warning! Takes a really long time to compute.\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # define the system constant parameters\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    max_iters = 20\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    best_rmse_te = np.ones(7)*999999\n",
    "    best_rmse_tr = np.ones(7)\n",
    "    best_method = np.ones(7)\n",
    "    best_degree = np.ones(7)\n",
    "    best_gamma = np.ones(7)\n",
    "    best_lambda = np.ones(7)\n",
    "    best_w0 = np.ones(7)\n",
    "    i = 0\n",
    "    \n",
    "    # define the system variable parameters\n",
    "    methods = np.linspace(0, 6, 7)\n",
    "    degrees = np.linspace(3, 7, 5)\n",
    "    gammas = np.logspace(-3, 0, 4)\n",
    "    lambdas = np.logspace(-3, 1, 5)\n",
    "    w0s = np.linspace(-5, 5, 3)\n",
    "    \n",
    "    for method in methods:\n",
    "        for degree in degrees:\n",
    "            for gamma in gammas:\n",
    "                for lambda_ in lambdas:\n",
    "                    for w0 in w0s:\n",
    "                        rmse_tr_tmp = []\n",
    "                        rmse_te_tmp = []\n",
    "                        for k in range(k_fold):\n",
    "                            loss_tr, loss_te, _ = cross_validation(y, x, k_indices, k, degree, gamma, lambda_, max_iters, method, w0)\n",
    "                            rmse_tr_tmp.append(loss_tr)\n",
    "                            rmse_te_tmp.append(loss_te)\n",
    "                        rmse_tr = np.mean(rmse_tr_tmp)\n",
    "                        rmse_te = np.mean(rmse_te_tmp)\n",
    "                        if rmse_te < best_rmse_te[int(method)]:\n",
    "                            best_rmse_te[int(method)] = rmse_te\n",
    "                            best_rmse_tr[int(method)] = rmse_tr\n",
    "                            best_method[int(method)] = method\n",
    "                            best_degree[int(method)] = degree\n",
    "                            best_gamma[int(method)] = gamma\n",
    "                            best_lambda[int(method)] = lambda_\n",
    "                            best_w0[int(method)] = w0\n",
    "                        i = i + 1\n",
    "                        print(str(i / (7 * 5 * 4 * 5 * 3) * 100) + \"% done\")\n",
    "    # print(\"Best rmse_te = %s ; Best rmse_tr = %s ; Best method = %s ; Best degree = %s ; Best gamma = %s ; Best lambda = %s ; Best w0 = %s\" % (best_rmse_te, best_rmse_tr, best_method, best_degree, best_gamma, best_lambda, best_w0))\n",
    "    print(best_rmse_te)\n",
    "    print(best_rmse_tr)\n",
    "    print(best_method)\n",
    "    print(best_degree)\n",
    "    print(best_gamma)\n",
    "    print(best_lambda)\n",
    "    print(best_w0)\n",
    "    return best_rmse_te, best_rmse_tr, best_method, best_degree, best_gamma, best_lambda, best_w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047619047619047616% done\n",
      "0.09523809523809523% done\n",
      "0.14285714285714285% done\n",
      "0.19047619047619047% done\n",
      "0.2380952380952381% done\n",
      "0.2857142857142857% done\n",
      "0.33333333333333337% done\n",
      "0.38095238095238093% done\n",
      "0.4285714285714286% done\n",
      "0.4761904761904762% done\n",
      "0.5238095238095238% done\n",
      "0.5714285714285714% done\n",
      "0.6190476190476191% done\n",
      "0.6666666666666667% done\n",
      "0.7142857142857143% done\n",
      "0.7619047619047619% done\n",
      "0.8095238095238094% done\n",
      "0.8571428571428572% done\n",
      "0.9047619047619048% done\n",
      "0.9523809523809524% done\n",
      "1.0% done\n",
      "1.0476190476190477% done\n",
      "1.0952380952380953% done\n",
      "1.1428571428571428% done\n",
      "1.1904761904761905% done\n",
      "1.2380952380952381% done\n",
      "1.2857142857142856% done\n",
      "1.3333333333333335% done\n",
      "1.380952380952381% done\n",
      "1.4285714285714286% done\n",
      "1.4761904761904763% done\n",
      "1.5238095238095237% done\n",
      "1.5714285714285716% done\n",
      "1.6190476190476188% done\n",
      "1.6666666666666667% done\n",
      "1.7142857142857144% done\n",
      "1.7619047619047619% done\n",
      "1.8095238095238095% done\n",
      "1.8571428571428572% done\n",
      "1.9047619047619049% done\n",
      "1.9523809523809523% done\n",
      "2.0% done\n",
      "2.047619047619048% done\n",
      "2.0952380952380953% done\n",
      "2.142857142857143% done\n",
      "2.1904761904761907% done\n",
      "2.238095238095238% done\n",
      "2.2857142857142856% done\n",
      "2.3333333333333335% done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e505c463fc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1210f5f59e22>\u001b[0m in \u001b[0;36mtest_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                             \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                             \u001b[0mrmse_tr_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                             \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ad2eef607134>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, degree, gamma, lambda_, max_iters, method, w0)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA3/The_Little_Turings/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# compute gradient and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# update w by gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA3/The_Little_Turings/ml_math.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w, loss_function)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# choose correct loss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MSE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MAE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_MAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA3/The_Little_Turings/ml_math.py\u001b[0m in \u001b[0;36mcompute_MSE\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mMSE_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMSE_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "rmse_te     rmse_tr      method     degree       gamma      lambda       w0\n",
    "1.802e-6    1.802e-6     0          4            1          -            0\n",
    "1.815e-6    1.841e-6     1          3            0.1        -            0\n",
    "6.171e-5    1.243e-6     2          3            -      -            -5\n",
    "1.800e-6    1.799e-6     3          3            -      1e-3         -5\n",
    "4.552e-5    4.997e-1     4          3            0.001      -            0\n",
    "3.122e-5    5.003e-1     5          4            0.001      10           0\n",
    "1.802e-6    1.802e-6     6          3            1          1e-3         0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Four methods give a close mean-square error of 1.8e-6. They correspond to the methods least-square gradient descent, least-square stochastic gradient descent, ridge-regression and ridge-regression gradient descent. Ridge-regression gives the best results though so let's try to have an even better mean-square error. The right degree seems to be 3 or 4 since all our results are in this range and it seems that the function wants a small gamma and lambda and likes the initial w0 to be shifted from the origin. Let's write a new test script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ridge_regression():\n",
    "    \"\"\"\n",
    "    ----------------------------------------------------------------------------\n",
    "    Function that try to find the best parameters for ridge regression.\n",
    "    ----------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # define the system constant parameters\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    max_iters = 20\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    best_rmse_te = 999999\n",
    "    best_rmse_tr = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    gamma = 0\n",
    "    method = 3\n",
    "    w0 = 0\n",
    "    \n",
    "    # define the system variable parameters\n",
    "    degrees = np.linspace(3, 4, 2)\n",
    "    lambdas = np.logspace(-6, 0, 20)\n",
    "    \n",
    "    for degree in degrees:\n",
    "        for lambda_ in lambdas:\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te, _ = cross_validation(y, x, k_indices, k, degree, gamma, lambda_, max_iters, method, w0)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "            if rmse_te[i] < best_rmse_te:\n",
    "                best_rmse_te = rmse_te[i]\n",
    "                best_rmse_tr = rmse_tr[i]\n",
    "                best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "            i = i + 1\n",
    "            print(str(i / (2 * 20) * 100) + \"% done\")\n",
    "        cross_validation_visualization(lambdas, \"lambda\", rmse_tr[int((j)*20):int((j+1)*20)], rmse_te[int((j)*20):int((j+1)*20)], \"log\",\"degree = \" + str(degree),([0, 0, j]),([1, 0, j]))     \n",
    "        j = j + 1 \n",
    "    print(best_rmse_te)\n",
    "    print(best_rmse_tr)\n",
    "    print(best_degree)\n",
    "    print(best_lambda)\n",
    "    return best_rmse_te, best_rmse_tr, best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ridge_regression()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We achieved to reduce the mean-square error to 1.306e-6 with the parameters degree = 3 and lambda = 1e-14."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
